\chapter{State of the Art}
The current state of the art can be divided into 3 categories, which are Crawling of Code\ref{sec:coc}, Infrastructure as Code Analysis\ref{sec:iaca}, and Recommender Systems for Developers\ref{sec:rsfd}. Those categories are chosen since there is no direct paper related to this thesis, but papers in each of those categories, which are whithin their categories related to this thesis.
% Related work. Present state of research and applied solutions concerning the different aspects relevant to the thesis. Discuss differences and similarities to other solutions to the given tackled problem. Approximately 10 to 15 pages.

% Länger als Background. State of the Art ~ 4-5 Seiten, aber wichtig Background immer weniger
% (0,5-1 Seite je paper), die sehr ähnlich sind zu deiner Arbeit. Weniger genau Paper, die teilaspekte deiner Arbeit beschreiben (ca. 0,5 Seite je paper). Zusammenfassned dann alle paper die perifär ähnliches beschreiben (typisch sind hier so 4-8 paper pro halbe seite.

\section{Crawling of Code}
\label{sec:coc}
In regards to "Crawling of Code", one of the papers is "Mining the Network of the Programmers: A Data-Driven Analysis of GitHub" \todo{cite} published by Ma et al., which has the goal of collecting public data of GitHub and analysing it as a social network. Within their work, they implemented a distributed crawler, which collected more than 2 million user data from the profile pages on GitHub. Their implementation used a scheduler connected to a MySQL database, which recorded the data, and worker nodes, which used a Breadth-First Search algorithm to crawl the users profile pages on GitHub by using the follower and followings lists. They applied machine learning to the collected data by selecting certain features like total amount of stars the user received or the amount of repositories the user owns and visualized their findings in charts.
Compared to this thesis they took the approach of a distributed crawler as well since it covers more data more quickly, but they had the advantage of only having to access publicly available data from the GitHub page without having to use their API and its limitations. Due to the usage of MySQL their findings lack the analysis of social network related metrics like the average shortest path, the clustering coefficient and its possibility of a small world problem, and many more, which could have been derived by using a graph or graph database. %Using a BFS algorithm compared to GitHub APi

The paper "Influence analysis of Github repositories" by Hu et al. deals with the analysis of important and influential GitHub repositories based on publicly available data. Using this data, they created a graph based on the stars a repository received over time by utilizing the GitHub star event. They used several sources to collect this data. One of those sources was the GitHub API to gather metadata on users and repositories. Another source is the "GH Archive"\footnote{https://www.gharchive.org/}, which offers all available event data from 2011 to 2020 and was used as a main driver of their data collection since GitHub itself only offers up to 300 events via their API with an maximum age of 90 days. The setup didn't require any distributed crawler logic, as the "GH Archive" offered all available data without the need to crawl data oneself. In direct comparison the solution using the "GH Archive" as a data source is a good starting point, but it only offers event data. There is no public repository containing a global index of all available files that are kept in public GitHub repositories.

A related master thesis "Crawling and Analyzing Repository in GitHub" by Zhongpei Zhang deals with the collection of user and repository data of all repositories with more than 500 stars of GitHub. The goal is to cluster the usage of programming languages according to repositories and users, and analyse the data and how programming languages might be coupled together depending on their usage. The thesis does not describe their crawler implementation nor does it describe the database they used, but they describe in detail the process of crawling GitHub, which shows similarities to this thesis in terms of limitations. Zhongpei Zhang heavily relied on the GitHub API in terms of crawling user and repository data, which are both endpoints that allow up to 5000 queries an hour compared to only 1800 queries per hour for the code API. Another similarity is the usage of query parameters to create unique queries and receive additional data compared to a standard search.

\section{Infrastructure as Code Analysis}
\label{sec:iaca}
The first paper in regards to the "Infrastructure as Code Analysis" is called "Cloud WorkBench – Infrastructure-as-Code Based Cloud Benchmarking" by Scheuner et al. and describes the creation of a cloud benchmarking Web service with focus on Infrastructure-as-a-Service clouds. Since cloud benchmarking can be error prone if done manually, an automation was required to verify validity of the results. For this the concept of Infrastructure as Code was used, since it verifies that the same resources are created if the same manifest was used and thereby providing reusability and consistency. The software Chef\footnote{https://www.chef.io/} was used as automation and orchestration system to run the benchmarks, which were previously defined in Infrastructure as Code manifests. Comparing this thesis and the paper, it shows similarities in goals and execution since both deal with the benchmarking of a related field in a consistent and automated way. While the paper deals with the benchmarking of cloud providers, this thesis deals with the benchmarking of Infrastructure as Code related deployment scripts. Both projects relied on the concept of Infrastructure as Code, since it provides a reusable and consistent way to execute benchmarks and an automation software, which deals with the execution.

The second paper "Testing Idempotence for Infrastructure as Code" by Hummer et al. deals with the testing of idempotency of Infrastructure as Code scripts with a focus on Chef. For this a distributed prototype was implemented, which tested 298 so-called cookbooks provided by the Opscode community. Their system consists of a Web interface, which controls the test execution, and various virtual machines, which are each running a test agent. This test agent is an implementation, that executes, intercepts tests, and inserts the results into a MongoDB. The tests itself are run within a Linux container (LXC), which offer proper environment isolation and allow parallel test execution. Overall the idea of this paper goes in a similar direction as this thesis, just with a different goal. Publicly available data was used, while the paper didn't require an additional crawler, due to the manual selection of deployment scripts according to different criteria. Test execution was done in form of a pipeline and proper workload isolation using the most popular tools at that time, which supports the approach in this thesis. While this paper focuses on the idempotence of deployment scripts it does not create a score in any sense, but rather whether a script behaves in the same way if executing multiple times, which could be used as an additional indicator for this thesis in case of the implementation of a Chef strategy.

An analysis of all available non-forked Dockerfiles was done in October 2016 and is described in the paper "An Empirical Analysis of the Docker Container Ecosystem on GitHub" by Cito et al. Instead of crawling public data via the GitHub API, a public GitHub archive was used, which is not further described nor properly explained how. The "GH Archive"\footnote{https://www.gharchive.org/} does not contain any data about the contents of a repository, except for event data. To gain additional metadata about the set of 70197 Dockerfiles the GitHub API was used to be able to connect the information of the Dockerfile better to such things as programming languages or project size. Their goal was to compare usage of Docker in popular projects to the general GitHub community. A relational database was used as storage. Their implementation involved a Java application, which applied a linter to each Dockerfile and its revisions and inserted the results into the relational database. A manual execution of 560 repositories was done, which resulted in 34\% not being able to build. The lack of version pinning resulted in linting errors for 28,6\% of all Dockerfiles. This analysis was done in 2016 and similar observations can still be done in 2020 in terms of version pinning. The paper solely focused on an overall analysis of all available Dockerfiles on GitHub, while this thesis focuses on deployment scripts and recommending those according to a calculated score. Ideas of this paper, like linting Dockerfiles, could be included in the future work or utilizing the same system to create strategies to execute all 70000 Dockerfiles in a consistent way.

\section{Recommender Systems for Developers}
\label{sec:rsfd}
"Knowledge-aware Recommender System for Software Development" is a paper written by Nguyen et al. and proposes a recommender system with a focus on open source software by utilizing a knowledge graph. This system shall help developers to find similar projects with the same focus or suggesting libraries that similar projects have used. In total they implemented all known types of recommender system, each for a different use case. Similar approaches were taken in regards to the knowledge graph, but besides that the use cases are not comparable since the paper focuses on Maven\footnote{https://maven.org}, a artifact repository.

The paper "A Declarative Recommender System for Cloud Infrastructure Services Selection" by Zhang et al. defines a recommender system for Infrastructure as a Service providers. Its recommendation is based on the cost calculation of user defined services, which those available providers offer. The system shall help a user to decide which provider to select depending on the use case they have. Their implementation consisted of a Web service using JavaScript frameworks as frontend and Java as backend utilizing a MySQL database. The recommender system can be describe as a knowledge recommendation, similar to this thesis, since it relies on user defined input to be able to recommend data.