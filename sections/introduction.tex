\chapter{Introduction}
%All the reader needs to know to get introduced to the topic. Motivate, state the problem and give a hint to your contribution. What is this thesis about? Why is it interesting? Give the reader a brief idea of the structure of the thesis. One to three pages.
Infrastructure as Code (IaC) has gained a lot of traction in recent years to declarative write and describe your system rather than individually configure each component of it.\\
This approach allows one to recreate their system consistently and without the problem of maintaining snowflakes.\\
Open source tools such as Docker (including swarm and compose), Terraform, Kubernetes, Ansible and OpenStack Heat can be used to define the infrastructure in so called manifests files and represent an abstraction of the actually used software. While those tools are helpful it still requires one to maintain the software within those definitions and their compatibility among each other.\\
Therefore the aim is to gather those manifests from various sources, e.g. GitHub, to build a knowledge graph of open source projects used, their version and configuration.\\
Based on the knowledge graph an analysis can be carried out to predict compatible version numbers and configurations for a given set of tools.\\
The last step will be to evaluate whether those predictions done in the previous step actually work by deploying those.

The early focus will rely on the crawler. The crawler will be able to work in a distributed matter, meaning master/worker scheme, but also be able to work standalone in case one does not want to deploy multiple instances. The communication between those instances will work in an asynchronous communication scheme for client/server so there will be no blocking messaging on either side of the component and will expose a unified REST API to execute crawling/processing/health status, etc.\\
In case the selected master instance is not reachable the worker will keep its progress and report back as soon as the master is reachable again.\\
The master in this case will act as information sink and process the results further and insert them into the database.\\
\\
As one of the tasks will be to build up a knowledge graph the database will be a graph database, that will help to further show the connection between different services and allow one to evaluate further. Possbile candidates for this are neo4j\footnote{https://neo4j.com} or grakn.ai\footnote{https://grakn.ai}.\\
\\
The crawler itself will use the strategy pattern to support different kind of crawling methods to either crawl GitHub or even Google, as Google does a lot of indexing and direct linking of all sorts of files that we are looking for. Another possible candidate may be Bitbucket, but for now the strategy pattern will allow a lot of flexibility for further implementations.\\
\\
The strategy for GitHub will build up on their API that allows to search for specific files as describe in their section about "Search Code"\footnote{https://developer.github.com/v3/search/\#search-code} and allows to crawl about 30.000 results per hour per valid GitHub Account before running into rate-limiting. At the time of writing they have about 800.000 results for "docker-compose.yml" files and therefore will require to either create dummy accounts or a different strategy. With each request the total count varies between 700.000 and 800.000 for the API as well as their web application.\\
The second strategy for GitHub is to crawl their web application and possibly circumvent the rate-limiting.\\
\\
The master and therefore processing side will adopt the strategy pattern as well to be able to handle all sort of files, whereas the first focus will rely on docker-compose files and their ecosystem.\\
\\
On the other hand a separate application will be needed, which will expose a web application to visualize the graph, but also work as an interface to interact with the crawler to send commands on what to crawl and process. This will also be used to analyse and evaluate a given set of tools.\\
For the evaluation a docker in docker (dind) image will be used, which exposes the docker daemon and allows to execute any docker-compose file, this will ensure the same conditions for all deployments and allows to easily vary the docker and docker-compose versions. This evaluation step could possibly be connected with a Jenkins to have a unified test pipeline to run some health checks against the deployment or just integrate it in the already existing application.\\