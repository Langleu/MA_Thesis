\chapter{Contribution}

This chapter is divided into multiple sections, which cover the architecture and the expandability of the implementation. The architecture is split into subsections covering the various components that were implemented throughout the thesis. Overall it should give the reader an understanding of the motivation and decisions done to implement the requirements.

%Most important chapter of the thesis. Describes what the author contributes as research. Discusses intuition, motivation, describes and reasons about necessity of proposed elements. Defines theses based on reasonable assumptions. Discusses relevant aspects of contribution. Approximately 30 to 40 pages. Can be split into multiple chapters.

\section{Architecture}

The architecture is in the most classical way a microservice orientated architecture. Compared to a monolithic application there is a higher focus on single components and their implementation and therefore decouple the complexity in its entirety.

Services itself can be clustered into the "Distributed Crawler", "Distributed Analysis", "Database", and "Frontend" sections, whereas the database is a third party implementation that is built upon and integrated into the architecture. The visualization of the microservice architecture can be seen in \ref{fig:architecture}.

\todo{redo image with clustered sections}
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{graphics/architecture_v2.png}
    \caption{Architectural representation of the system as a whole}
    \label{fig:architecture}
\end{figure}

\subsection{Database - Grakn.ai}
One of the fundamental decisions is which database to select as a lot of other components will have to build upon it. From the beginning on it was clear that for building a knowledge graph some sort of graph database could be utilized as it will clearly show the relations of all entities and help in building a recommender system due to the sheer amount of data in the graph.
The implementation could have been done with a NoSQL or a classical SQL database as well, but would have required more linking of data to represent the same sort of result using a graph database.

The first candidate for a graph database that comes to mind is neo4j\footnote{https://neo4j.com}, which is one of the older ones publicly released in 2007\todo{quote https://neo4j.com/developer/graph-database/\#neo4j-overview}. While neo4j could have been used for the implementation, it felt quite outdated when it came to the data representation and their query language "Cypher". Therefore the graph database Grakn\footnote{https://grakn.ai} was used, which was released in 2016\todo{quote source }.

Grakn describes itself as a knowledge graph engine to organize complex networks of data and making it queryable, by performing knowledge engineering\todo{quote https://grakn.ai/grakn-core}. It fully supports the Entity-Relationship model and comes along with a query language called Graql. 

\todo{Deduplication}
\subsubsection{Entity-Relationship model}
Entity-Relationship models are the de facto standard to represent a schema for a graph database.
\todo{redo er model}
\begin{figure}[H]
    \centering
    \includegraphics[width=1.2\paperwidth,height=1.2\paperheight,keepaspectratio,angle=270]{graphics/er_database.png}
    \caption{Entity-Relationship model}
    \label{fig:er_model}
\end{figure}

Deployment Scripts do not follow a standard and therefore a generalized model has to be created that possibly fit most of them. The focus was lying on the deployment as this entity represents the script itself as a whole. Followed by services that are defined in the deployment script and therefore have a direct relationship to the deployment. Services can occur unlimited times in a deployment, but one service can always only have one deployment. This was kept this way as services, depending on the context you are in, can be differently defined when it comes to versions or extra attributes, but as we are dealing with a graph database searching for possibly best fits will still work easy as one has the direct relationship between services and its deployment.
Services can depend on itself as well, meaning that those are more coupled than others, e.g. a database and a backend for example. Once more this depends highly on the context one is in and will be part of another section in this chapter. \todo{e.g. kustomize, kuberentes, helm, docker-compose fit this schema}
As the deployment script is usually embedded in a repository a coupling is happening there as well. The repository could represent in a classical way a repository on either GitHub or Bitbucket, but could also be a website or documentation depending on where it was found. One repository can hold an unlimited amount of deployment scripts and one can possibly belong to multiple repositories. The simple explanation is that the secure hash algorithm is used as key identification and files that have the same content will therefore have relationships added to those repositories as well, while still keeping only one deployment entity in the database. The last entity in the schema is a user that owns unlimited repositories, while one repository can only belong to one user.

One of the favorable aspects of Grakn is how it handles duplicates. As long as one defines unique keys in the attributes of an entity there will not be a duplicate as those will automatically be dropped on insertion. This will keep the database free from duplicates and allows one to concentrate on the actual implementation of their use case.

The schema is kept general in regards to the domain of different kinds of deployment scripts while overall favouring the origin from code repositories. A visualization of the schema can be seen in figure \ref{fig:er_model} and gives a self explanatory overview according to current standards in ER models.

\subsubsection{Open Source Contributions}
Grakn is still quite a new graph database due to which the community around it is still rather small and problems that have been solved for others still have to covered for this one. Therefore some contributions to the project around Grakn and its tools have been covered in the implementation of this thesis.

First of all Grakn comes along with tool called workbase, which is essentially an editor to show and edit the graph that the database contains. This is sufficient for most projects where the graph is rather small. The workbase is a Vue/React application wrapped in electron, which is a framework to create desktop applications from JavaScript. Therefore electron is wrapping the files with chromium and node.js to display the application cross platform. The workbase comes along with some flaws as graphs with nodes and edges in the 100.000 area will crash the application due to the limited amount of memory assigned to the chromium process and even in the area of 10.000 and more the application starts to stutter under the heavy load.

Therefore the first contribution in the domain of Grakn is a converter from Grakn to the the Graph Exchange XML Format (GEXF), which is essentially a file format to describe complex networks structures.
\todo{e.g. own tooling for
creating gexf compatible graphs - Limitations of provided tools (electron)}
\todo{e.g. own tooling for seeding the db model}

\subsection{Distributed Crawler}
First and foremost the main part of the implementation is the so-called "distributed crawler", which follows the master and node pattern, where one entity is the master that controls the spawned nodes. In this thesis, it can also be seen as orchestrator/information funnel and the crawling/processing unit. The reason for calling it a distributed crawler is that the overall function of this implementation can be seen as a crawler and distributed in the sense that the nodes can be distributed across multiple data centers or even be used on edge nodes with the appropriate implementation.

The implementation was done using Node.js, which is an asynchronous JavaScript runtime, which allows to run JavaScript outside of the browser and potentially build scalable applications. One of the big advantages of Node.js is that you can use the synergies of running JavaScript on the server-side and client-side. While this might not be the first thing that comes to mind when thinking about a distributed crawler, it does come in handy as a lot of services offer an API\footnote{Application Programming Interface} that returns JSON\footnote{JavaScript Object Notation} objects. Those JSON objects are easier to deal with in its native environment compared to other programming languages where one would rely on a third party implementation and a lot of serialization as the implementation was simply not meant for that environment.
As JavaScript runs in a JavaScript engine, the V8 engine developed by Google, it can be extended by creating C++ modules to make certain functions quicker as those would natively run on the system.

The master and crawler nodes are all based on one unified code base and the only difference between the two is the addition of the database connection and the orchestration routes, which are both parts of the master node. The default startup parameter for a node is to act as a crawler and connect to the master, which can be changed by setting one environment variable called "TYPE" to "master". The node would therefore load the database module, orchestration routes and use the master implementation of the communication channel instead of the crawler one.
The benefits of a unified code base are that the application could be extended in the future to support e.g. leader election or let the master act as a crawler as well. Meaning depending on the processing power provided the master could handle on top of its own capabilities the ones of a crawler as well and therefore provide more coverage of crawled sources.
Leader election quickly explained is that e.g. 4 spawned crawler would elect one of their own to be the master node, which often works based on either first come first serves basis or other defined constraints.

Master and node in itself are on a fundamental level just web servers providing a REST\footnote{Representational State Transfer} API, that allows one to trigger functions either from outside or from the application itself. REST comes along with well-defined methods using "GET", "POST", "DELETE", "PATCH", and some others. In the context of the distributed crawler, the most used methods are "GET" and "POST", as one either wants to receive information or in a sense add information, while often "POST" is used as well to incorporate functions that are not covered by the other methods.
While a REST API grants a good interface for a user or another application it is not very well suited for bidirectional real-time communication. As the crawlers are potentially distributed over various data-centers it can be tough to keep track of every single one of them, especially for the master as one would need e.g. a domain or another unique identifier to connect to all nodes prior to deploying the master. To counter this issue another communication channel was introduced the so-called WebSockets, which allows a real-time bidirectional communication where only the crawler would need to know the unique identifier of the master instead of the other way around. WebSockets initially coming from the web browser domain to allow client-server communication can also be used as a pure client-server communication without a web browser due to Node.js . WebSockets use a TCP socket to communicate through and are on the same OSI layer as HTTP and make use of channels that client and server have to subscribe to, to be able to send and receive messages about a specific topic. This bidirectional communication channel will be part of further subsections.

\subsubsection{Information funnel}
One of the functions of the master node is to act as an information funnel or proxy to the database. This makes the implementation of the crawler nodes easier as one does not have to worry about connecting to the database and can just use the bidirectional communication channel with the master. From a security point of view, it might also be easier to keep that one communication channel secure compared to multiple small ones to the database from depending on the setup of multiple locations.

Crawlers are essentially preparing and processing what they have crawled completely on their side and send the finished result to the master, which in return inserts the results into the database. The established bidirectional communication channel will be used keeping additional overhead small compared to other ways of implementing it.

\subsubsection{Orchestration}
As the master and node pattern already suggests controlling is one of the aspects of a master node. In this case, the job orchestration will be handled from the master node. Therefore the master provides a REST API that can be consumed by e.g. a user or some sort of automation like cron jobs that trigger crawling on a weekly basis. This could have been implemented using WebSockets as well, but a REST API is easier to consume compared to establishing a socket connection and then sending a message for a specific topic.

While orchestration is a REST API route it utilizes the WebSocket to send messages to its connected nodes and uses those as well for calculating windows that have to be crawled. A window in this sense is a range where the crawling should start and end, but more about the exact reasoning in the subsection about crawling. The orchestrator has a minimum and maximum value in between which should be crawled, which then in return is divided by the amount of crawler that are available at the point of receiving the request for orchestration. Each crawler therefore receives a window that should be covered.

\subsubsection{Crawling}
The implementation of the crawler makes use of the strategy pattern, which is a behavioral software pattern that allows selecting a strategy/implementation during runtime. This means in particular that there is a generic interface with the defined functions that have to be implemented by each strategy. In Node.js there is no classical object orientation that a lot of people might know from Java. Therefore the implementation makes use of a class that implements the generic interface that then each strategy can extend and implement. Further on there is one main class, which initializes the required strategy during runtime by making use of a simple switch case.

While this gives one a lot of flexibility and options to further extend this with other strategies it also brings along a couple of negative aspects like having to know which strategies are available and some extra effort into handling the implementation.

The first data source to be crawled and therefore the first strategy was for GitHub. GitHub is the biggest collection of public repositories and therefore offers a good starting point to cover roughly over a million deployment scripts in the case of docker-compose.
GitHub offers an API already in its third iteration with various REST endpoints that return JSON. One of those endpoints allows one to search through the code of all public repositories and therefore providing the starting point for the GitHub crawler strategy. As GitHub is quite popular and offers its API for free some of those API routes come with limitations, especially in something so resource-intensive as searching code through all public repositories.
Therefore the first limitation that GitHub introduced for the "search REST endpoint" is that only authorized users can use this function and with that, the second limitation comes along that requests will be limited to 30 per minute. To circumvent those two limitations each crawler gets assigned a designated GitHub Account with username and token to authenticate against the API and in case of running into the 30 requests per minute limit the crawler will sleep in intervals till the limit is lifted again.
The next limitation is that GitHub will only return a maximum of 1000 results per search query. While this seems difficult to bypass, GitHub brings along the proper tools to do so. One of the query parameters of the "GET" request is simply called "q" and allows one to build complex individual queries that are unique in itself and therefore offer up to 1000 results per query. The query parameter "q" can be extended to include the file name and file extension. Deployment scripts are often written in YAML\footnote{YAML Ain't Markup Language} and have come along with the extension ending "yaml" and "yml". Therefore doubles the number of search results in the case of YAML specific deployment scripts as one can vary between those two different spellings. The most important factor for circumventing the limit of 1000 results is to utilize the size of a file. GitHub allows one to define the size of a file, that you are searching for, in bytes or in a byte range, meaning one could search for all files in a range of 100-102 bytes or simple in the use case of the thesis to search per byte. This would allow one to create for the same file a unique query every single time.
Of course, GitHub has a limitation for this one as well and only allows to find files up to 384 Kilobyte, which in the context of this thesis is negligible as deployment scripts rarely touch the two-digit Kilobyte area.

Theoretically speaking 384 million files could be covered by just utilizing the size factor without even utilizing such things as extensions, order, or sorting. With those included depending on the type of deployment script, one could easily cover up to a billion search results.

To summarize the GitHub API, it comes along with a lot of limitations but at the same time with all tools needed to circumvent those limitations and therefore covering almost all public repositories.

This should give one a perspective as well on why one would utilize a distributed crawler as this allows to cover a much wider area much quicker by spawning more nodes. As previously mentioned in the part about orchestration a window area is defined for crawling, this is limited from 50 bytes to 384 Kilobytes as a 50 byte deployment script will most likely just include the very basics of its potential. One crawler does not cover the full range, except its the only crawler available, but rather the window that was set by the master node.

\subsubsection{Processing}


\subsubsection{Unified base}
\todo{meant as the same code base for client/server - interchangeable}
\subsection{Distributed Analysis}
\subsubsection{Choice of Tooling}
\todo{e.g. Jenkins + Kubernetes, conftest}
\subsubsection{Importance of Isolation}
\todo{e.g. CI and dind or kind}
\subsubsection{Vulnerability Scanning}
\subsubsection{Calculation of Score}
\subsubsection{Orchestrator / Funnel}
\todo{Normalization}
\todo{CVSS}
\subsection{Frontend}
\subsubsection{Content-based Recommendation}
\section{Expandability}
\todo{current focus on docker-compose, but can easily be extended to further sources, may it be crawling/processing/analysing/showing}
\section{IaC of implementation}

%\section{Open Source}
%\subsection{GitHub}

%\section{Architecture}
%\subsection{Microservices}
%\subsection{Software design pattern}
%\todo{Strategy pattern}
%\todo{Singleton}
%\subsection{Node.js}
%\subsection{React}

%\section{Protocols}
%\subsection{WebSocket}
%\subsection{REST}
%\subsection{gRPC}
%\subsection{JNLP}