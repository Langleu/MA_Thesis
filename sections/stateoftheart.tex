\chapter{State of the Art}
\label{sec:stateofart}
The current state of the art can be divided into 4 categories, which are Crawling of Code \ref{sec:coc}, Infrastructure as Code Analysis \ref{sec:iaca}, and Recommender Systems for Developers \ref{sec:rsfd}, and Code Analysis \ref{sec:codeAnalysis}. Those categories are chosen since there is no direct paper related to this thesis, but papers in each of those categories, which are within their categories related to this thesis.
% Related work. Present state of research and applied solutions concerning the different aspects relevant to the thesis. Discuss differences and similarities to other solutions to the given tackled problem. Approximately 10 to 15 pages.

% Länger als Background. State of the Art ~ 4-5 Seiten, aber wichtig Background immer weniger
% (0,5-1 Seite je paper), die sehr ähnlich sind zu deiner Arbeit. Weniger genau Paper, die teilaspekte deiner Arbeit beschreiben (ca. 0,5 Seite je paper). Zusammenfassned dann alle paper die perifär ähnliches beschreiben (typisch sind hier so 4-8 paper pro halbe seite.

\section{Crawling of Code}
\label{sec:coc}
In regards to "Crawling of Code", one of the papers is "Mining the Network of the Programmers: A Data-Driven Analysis of GitHub" \cite{mining} published by Ma et al., which has the goal of collecting public data of GitHub and analysing it as a social network. Within their work, they implemented a distributed crawler, which collected more than 2 million user data from the profile pages on GitHub. Their implementation used a scheduler connected to a MySQL database, which recorded the data, and worker nodes, which used a Breadth-First Search algorithm to crawl the users' profile pages on GitHub by using the follower and followings lists. They applied machine learning to the collected data by selecting certain features like the total amount of stars the user received or the number of repositories the user owns and visualized their findings in charts.
Compared to this thesis they took the approach of a distributed crawler as well since it covers more data more quickly, but they had the advantage of only having to access publicly available data from the GitHub page without having to use their API and its limitations. Due to the usage of MySQL, their findings lack the analysis of social network-related metrics like the average shortest path, the clustering coefficient and its possibility of a small world problem, and many more, which could have been derived by using a graph or graph database. Using a BFS algorithm compared to GitHub API to find possible repositories that own a "docker-compose" file is not feasible, since there are more than 40 million public repositories\cite{githubpublicrepos}, which could potentially contain such files. Crawling each repository would require one to crawl through each possible directory or once more use the GitHub API, this would likely result in an IP address blockage since a GitHub account is not required. In regards to the code collection, this thesis is the most related one in terms of the approach, since a distributed crawler was used.

The paper "Influence analysis of Github repositories" \cite{influencegithub} by Hu et al. deals with the analysis of important and influential GitHub repositories based on publicly available data. Using this data, they created a graph based on the stars a repository received over time by utilizing the GitHub star event. They used several sources to collect this data. One of those sources was the GitHub API to gather metadata on users and repositories. Another source is the "GH Archive"\footnote{https://www.gharchive.org/}, which offers all available event data from 2011 to 2020 and was used as the main driver of their data collection since GitHub itself only offers up to 300 events via their API with a maximum age of 90 days. The setup didn't require any distributed crawler logic, as the "GH Archive" offered all available data without the need to crawl data oneself. In direct comparison, the solution using the "GH Archive" as a data source is a good starting point, but it only offers event data. There is no public repository containing a global index of all available files that are kept in public GitHub repositories. The difference in the approach relies on the usage of a pre-collected data source, which as previously stated is not available for the data, which was required for this thesis. In terms of data storage, a similar approach can be assumed to be used since it was no further described, except for using a database and extracting a graph out of it. While this can be achieved by using a SQL or NoSQL database, the advantages of a graph database overshadow the extra steps required to using a pure SQL or NoSQL database.

A related master thesis "Crawling and Analyzing Repository in GitHub" \cite{Zhang2016CrawlingAA} by Zhongpei Zhang deals with the collection of user and repository data of all repositories with more than 500 stars of GitHub. The goal is to cluster the usage of programming languages according to repositories and users and analyse the data and how programming languages might be coupled together depending on their usage. The thesis does not describe their crawler implementation nor does it describe the database they used, but they describe in detail the process of crawling GitHub, which shows similarities to this thesis in terms of limitations. Zhongpei Zhang heavily relied on the GitHub API in terms of crawling user and repository data, which are both endpoints that allow up to 5,000 queries an hour compared to only 1,800 queries per hour for the code API. Another similarity is the usage of query parameters to create unique queries and receives additional data compared to a standard search.

The complementary use of the GitHub API and a GitHub event collection service seems to be a prevalent case for researchers. The same case for the paper "The quest for open source projects that use UML: mining GitHub" \cite{0.1145/2976767.2976778} by Hebig et al., which looks at the usage of UML in the context of open source projects. For this, the GitHub archive of GHTorrent\footnote{https://ghtorrent.org/} was used, which contains event data from 2012 to 2019, and contemplated with the GitHub API to get additional metadata. The potential problem of using such an archive is outdated data since the repository containing the related files could be private by then or the file has moved to a different location, possibly causing one to for non-existing files and exhausting the GitHub rate limit of 5,000 requests. In their paper, they state that they needed over 2 weeks to successfully crawl all metadata of 1,240,00 UML model files. While they don't state whether multiple GitHub accounts were used for the retrieval, they do mention later that for actual file analysis a total of 21 GitHub accounts were used. Since the file analysis took 6 weeks and the metadata retrieval over 2 weeks it can be assumed that parallelization was not used for the metadata retrieval and at the same time raises the question why 21 GitHub accounts were necessary for cloning public repositories. Overall the paper presented an interesting approach on the data collection of GitHub, which could be partly adopted for this thesis as well but still, a distributed crawler could make sense if launching this thesis as a longterm project since the system is expandability and allows further integration due to its REST API.

\section{Infrastructure as Code Analysis}
\label{sec:iaca}
The first paper in regards to the "Infrastructure as Code Analysis" is called "Cloud WorkBench – Infrastructure-as-Code Based Cloud Benchmarking" \cite{cloudworkbench} by Scheuner et al. and describes the creation of a cloud benchmarking Web service with a focus on Infrastructure-as-a-Service clouds. Since cloud benchmarking can be error-prone if done manually, automation was required to verify the validity of the results. For this, the concept of Infrastructure as Code was used, since it verifies that the same resources are created if the same manifest was used and thereby providing reusability and consistency. The software Chef\footnote{https://www.chef.io/} was used as automation and orchestration system to run the benchmarks, which were previously defined in Infrastructure as Code manifests. Comparing this thesis and the paper, it shows similarities in goals and execution since both deal with the benchmarking of a related field in a consistent and automated way. While the paper deals with the benchmarking of cloud providers, this thesis deals with the benchmarking of Infrastructure as Code related deployment scripts. Both projects relied on the concept of Infrastructure as Code since it provides a reusable and consistent way to execute benchmarks with the usage of automation software, which deals with the execution.

The second paper "Testing Idempotence for Infrastructure as Code" \cite{idempotence} by Hummer et al. deals with the testing of the idempotency of Infrastructure as Code scripts with a focus on Chef. For this, a distributed prototype was implemented, which tested 298 so-called cookbooks provided by the Opscode community. Their system consists of a Web interface, which controls the test execution, and various virtual machines, which are each running a test agent. This test agent is an implementation, that executes, intercepts tests, and inserts the results into a MongoDB. The tests itself are run within a Linux container (LXC), which offer proper environment isolation and allow parallel test execution. Overall the idea of this paper goes in a similar direction as this thesis, just with a different goal. Publicly available data was used, while the paper didn't require an additional crawler, due to the manual selection of deployment scripts according to different criteria. Test execution was done in form of a pipeline and proper workload isolation using the most popular tools at that time, which supports the approach in this thesis. While this paper focuses on the idempotence of deployment scripts it does not create a score in any sense, but rather whether a script behaves in the same way if executing multiple times, which could be used as an additional indicator for this thesis in case of the implementation of a Chef strategy.

An analysis of all available non-forked Dockerfiles was done in October 2016 and is described in the paper "An Empirical Analysis of the Docker Container Ecosystem on GitHub" \cite{empirical} by Cito et al. Instead of crawling public data via the GitHub API, a public GitHub archive was used, which is not further described nor properly explained how. The "GH Archive"\footnote{https://www.gharchive.org/} does not contain any data about the contents of a repository, except for event data. To gain additional metadata about the set of 70,197 Dockerfiles the GitHub API was used to be able to connect the information of the Dockerfile better to such things as programming languages or project size. Their goal was to compare the usage of Docker in popular projects to the general GitHub community. A relational database was used as storage. Their implementation involved a Java application, which applied a linter to each Dockerfile and its revisions and inserted the results into the relational database. Manual execution of 560 repositories was done, which resulted in 34\% not being able to build. The lack of version pinning resulted in linting errors for 28.6\% of all Dockerfiles. This analysis was done in 2016 and similar observations can still be done in 2020 in terms of version pinning. The approach was done in a similar way, but with the focus on Dockerfiles instead of "docker-compose" files, while the paper does not try to calculate a score, but rather just express in a binary way, whether a manifest follows certain community-defined best practices or not. Additionally, a distributed system was not used, which could have eased the manual testing and possibly enabled the testing of 70,000 Dockerfiles. Therefore, the paper solely focuses on an overall analysis of all available Dockerfiles on GitHub, while this thesis focuses on deployment scripts and recommending those according to a calculated score. Ideas of this paper, like linting Dockerfiles, could be included in the future work or utilizing the same system to create strategies to execute all 70,000 Dockerfiles in a consistent way.

There is no known paper that deals with the scoring of Infrastructure as Code related manifests. All of the papers presented deal with one aspect in particular, which could in return be used as an indicator of how good a deployment script is in terms of a final score. Overall the presented papers show similar approaches in terms of workload isolation and automation since those attributes are required to verify consistent results.

\section{Recommender Systems for Developers}
\label{sec:rsfd}
"Knowledge-aware Recommender System for Software Development" \cite{Nguyen2018KnowledgeawareRS} is a paper written by Nguyen et al. and proposes a recommender system with a focus on open-source software by utilizing a knowledge graph. This system shall help developers to find similar projects with the same focus or suggesting libraries that similar projects have used. In total, they implemented all known types of a recommender system, each for a different use case. Similar approaches were taken in regards to the knowledge graph, but besides that, the use cases are not comparable since the paper focuses on Maven\footnote{https://maven.org}, an artifact repository.

The paper "A Declarative Recommender System for Cloud Infrastructure Services Selection" \cite{costcalculatorcloud} by Zhang et al. defines a recommender system for Infrastructure as a Service provider. Its recommendation is based on the cost calculation of user-defined services, which those available providers offer. The system shall help a user to decide which provider to select depending on the use case they have. Their implementation consisted of a Web service using JavaScript frameworks as frontend and Java as backend utilizing a MySQL database. The recommender system can be described as a knowledge recommendation, similar to this thesis since it relies on user-defined input to be able to recommend data. Since the data does not resemble a possible social network, nor have any important relations, a relational database was sufficient. A data crawler was not required as well since the data has to be manually collected from the cloud providers public documentation. A score calculation does not take place, since the Web service will return the cost calculations for all cloud providers and a user has to decide and compare themselves between those. A knowledge-based recommendation was used as it relies on user-defined data, similar to this thesis, but has a shortage in actually recommending a final item since all options are presented to the user, which still require the user to review all items to determine themselves the best candidate.

"Prompter: A Self-confident Recommender System" \cite{6976143} by Ponzanelli et al. describes an IDE plugin called Prompter, that supports developers by automatically checking Stack Overflow discussions based on the context the developer is currently in and suggests those if a certain confidence is reached. It silently observes the context in the IDE and searches the Web and Stack Overflow discussions and evaluates the relevance of the results based on multiple factors. Those factors are code, conceptual, and Stack Overflow community related, which together result in a numerical value, which will trigger a notification if a user set threshold is reached. Thereby, this paper fits not only in the recommender system category but also the code analysis, since it not only described a recommender system but also a statical code analysis. Similar to this thesis their approach is based on crowdsourced data from multiple sources and use a knowledge-based recommendation system, which makes use of a self calculated numerical value established through multiple aspects. While their implementation is packaged as an IDE plugin, this thesis provides a Web service.

\section{Code Analysis}
\label{sec:codeAnalysis}
Code analysis, in particular static code analysis, is a useful method to assure a certain code quality or find errors before even running a program. While this thesis does not analyse source code of programming languages, it does still statically analyse the deployment scripts for best practices and prior to this even for compatibility of the file schema.
"Empirical Analysis of Static Code Metrics for Predicting Risk Scores in Android Applications" \cite{androidAlenezi} by Alenezi and Almomani conducted an empirical study on the impact of static code metrics and their relation to security vulnerabilities in Android applications. They analysed 1407 pre-selected Android applications by statically analysing the code using Sonarqube and used Androrisk to receive a risk score using fuzzy logic. Furthermore, they categorized the risk scores into No, Low, Medium, and High for an easier prediction model. By using the Spearman's Rank Correlation Coefficient they showed a direct correlation of static code violations to a higher risk score and used this combined with machine learning to classify other applications as well based on this data set. Taking this direct correlation into account it can be observed in deployment scripts as well, since the 5 deployment scripts with the highest score, as seen in Table \ref{images_with_highest_score}, were the ones with the least amount of vulnerabilities and following the best practices, which were derived by applying a static code analysis. To conclude a clearer picture an empirical analysis would need to be carried out on the existing crawled data, but the subset presented in Table \ref{images_with_highest_score} does lead to this assumption.

The paper "Context Is King: The Developer Perspective on the Usage of Static Analysis Tools" \cite{8330195} by Vassallo et al. carried out a study on the usage of static analysis tools in different contexts. Research questions covered context usage, configuration, and warning awareness in different contexts. Out of the 42 study participants, 37\% use static analysis tools in Continuous Integration, 33\% in Code Reviews, and 30\% in local programming environments. In terms of configuration 51\% only configure the static analysis tool once. Depending on the context developers inspect different warnings. Code reviews being more on the style and code redundancy site, local programming is more about code structure and logic, and CI is mostly to observe concurrency and handling errors.
This paper showed how deeply static analysis tools are embedded into the development flow of software for different use cases, which can be seen as one of the reasons to use such metrics in the calculation of the final score in this thesis. Those are represented by the static analysis of the best practices and the parsing of the deployment script, which could be summarized as code style checks, whereas the vulnerability scanner is a static analysis of the used images, reporting possible vulnerability and attack surface.