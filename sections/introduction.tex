\chapter{Introduction}
%All the reader needs to know to get introduced to the topic. Motivate, state the problem and give a hint to your contribution. What is this thesis about? Why is it interesting? Give the reader a brief idea of the structure of the thesis. One to three pages.
Infrastructure as Code (IaC) has gained a lot of traction in recent years due to the declarative writing and describing one's system rather than individually configuring each component of it. This approach allows the recreation of system consistently and without the issues of maintaining snowflakes. Open-source tools such as Docker-Compose, Terraform, Kubernetes, Ansible, Chef, Puppet, and OpenStack Heat can be used to define the infrastructure in so-called manifests files and represent an abstraction of the actually used software. According to Puppet in their 2020 "State of the DevOps Report" approximately 40,000 companies, including more than 75\% of Fortune 100 companies, have already adopted their tooling \cite{puppet}, which also means the adoption of Infrastructure as Code. Overall the number will be much higher since there are a lot of different tools that can be used to adopt the concept.

While those tools are helpful it still requires one to maintain the software within those definitions and their compatibility among each other.
Therefore the aim is to gather those manifests from various sources, e.g. GitHub, to build a knowledge graph of used open source projects, their version, and configurations. Those account to approximately 127,000 users, 160,000 repositories, 139,000 "docker-compose" scripts, which covered 409,000 services, and resulting in a total amount of 900,000 edges and 840,000 nodes.

Based on the knowledge graph an analysis can be carried out to predict compatible version numbers and configurations for a given set of tools by creating a knowledge-based recommender system, which utilizes a pre-calculated score of each manifest. The calculation of this score will be carried out in a distributed manner utilizing a standardized pipeline and isolated environment to ensure consistency across the results.
The recommender system is useful for DevOps Engineers and related fields to find well-designed deployment scripts based on their defined input, which shall help to create properly defined deployment scripts and be inspired by possible use cases. In the case of research, an analysis shall provide further insights into how specific deployment scripts are used by communities and due to the usage of a knowledge graph implicit relations can be extracted.

GitHub, being the biggest public version control, is the starting point for a distributed crawler. Since there is no publicly available data set containing contents of all publicly available repositories, a crawler had to be implemented. The crawler works in a distributed way by utilizing a master/worker scheme. The worker node consists of a crawling and processing unit, which both take advantage of the strategy pattern to provide a future proof implementation by creating new strategies, both in terms of crawlable sources and Infrastructure as Code tools. The master node works as an information sink and proxy to provide access to the database and orchestrates the worker nodes, all of those running in a scalable Kubernetes environment.
As one of the tasks will be to build a knowledge graph, the database will be a graph database, that will help to show the connections between different services and allow one to evaluate further on a visual level. For this, the graph database Grakn.ai was selected.

A distributed analysis will be carried out to calculate a score for a manifest by utilizing Jenkins and Kubernetes. The test pipeline consists of executing the manifest in an isolated environment, running best practices against it, analysing possible vulnerabilities by utilizing a state of the art vulnerability scanner, and finally calculating a score according to the significance of each collected metric.

While related work has always dealt with one aspect of a deployment script and providing a binary score of whether the analysed aspect is true or false, this thesis aims to provide a numerical score to use for a knowledge-based recommender system. With a focus on Docker-Compose, it can be further analysed how the community on GitHub uses the tool and which resources might implicitly be used often by utilizing the Secure Hash Algorithm (SHA) to link related scripts to each other.

The thesis consists of the next following 4 chapters, which are the \hyperref[sec:background]{Background}, \hyperref[sec:contribution]{Contribution}, \hyperref[sec:evaluation]{Evaluation}, \hyperref[sec:stateofart]{State of the Art}, and \hyperref[sec:conclusion]{Conclusion}. The Background consists of the required topics to understand the problem, motivation, and contribution. The chapter about the Contribution covers in-depth the implementation and reasoning behind the Distributed Crawler, Distributed Analysis, and Frontend. The carried out analysis and its results are described in the Evaluation and divided into the general analysis regarding metrical values and a graphical analysis derived by the knowledge graph. State of the Art covers related work and discusses similar approaches and differences between their work and this thesis. The Conclusion summarizes the most important findings and gives an outlook on future work.